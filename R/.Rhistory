pooledChain2 <- thinned2[[1]]
for (i in 2:length(thinned2)) pooledChain2 <-
rbind(pooledChain2, thinned2[[i]])
# Getting the sampled HPD from the chain:
ltReg2 <- pooledChain2[,2]
allHPD2 <- pooledChain2[which(ltReg2==max(ltReg2)),][-1]
HPD2 <- allHPD2[-5]
plot(tempdata$NT,tempdata$Temp, col=adjustcolor('black', alpha=0.2), pch=16)
lines(seq(0,1,length=1000), yhat(seq(0,1, length=1000), HPD2))
lines(seq(0,1,length=1000), yhat(seq(0,1, length=1000),
c(HPD2[1:3], HPD2[4]+exp(allHPD2[5]))), col='red')
hist(exp(thinned[[2]][,6]))
set.seed(1576)
ModelChain <- pMH(proposal=multvarNormProp,
# start the chain in with optim estimate of covariance:
propPars=proposalCov,
lTarg=lmodelTarget,
lTargPars=logTargetParameters,
# use the regression optim output as the initial start to
# the MCMC chain:
x0=c(allHPD,tempEst),
itermax=itermax,
uFunc=multvar.uFunc,
nChains=4,
# Some of our functions call other functions within them
# and so we need to pass these to each of the
# clusters:
clNeeds = c('rmvnorm','llikModel','yhat','getMinMax'))
thinned2 <- thinMCMC(ModelChain, alpha=0.1)
thinned2nl <- lapply(thinned2, function(x) x[,-1])
combined <- thinned2nl %>% lapply(FUN=mcmc) %>% do.call(what=mcmc.list)
gelman.plot(combined)
plot(thinned2, names=c("log Posterior", "A", "B", "C", "D", "Sigma", "DayTemp"),
lag.max=10)
# combining all the chains together:
pooledChain2 <- thinned2[[1]]
for (i in 2:length(thinned2)) pooledChain2 <-
rbind(pooledChain2, thinned2[[i]])
# Getting the sampled HPD from the chain:
ltReg2 <- pooledChain2[,2]
allHPD2 <- pooledChain2[which(ltReg2==max(ltReg2)),][-1]
HPD2 <- allHPD2[-5]
plot(tempdata$NT,tempdata$Temp, col=adjustcolor('black', alpha=0.2), pch=16)
lines(seq(0,1,length=1000), yhat(seq(0,1, length=1000), HPD2))
hist(exp(thinned[[2]][,6]))
hist(exp(thinned2[[2]][,6]))
HPD
plot(tempdata$NT,tempdata$Temp, col=adjustcolor('black', alpha=0.2), pch=16)
lines(seq(0,1,length=1000), yhat(seq(0,1, length=1000), HPD))
# plot the Highest Posterior Density curve:
plot(tempdata$NT,tempdata$Temp, col=adjustcolor('black', alpha=0.2), pch=16)
lines(seq(0,1,length=1000), yhat(seq(0,1, length=1000), HPD))
library(ggplot2)    # for plots
library(magrittr)   # for pipes
library(mvtnorm)    # for multivariate gaussian proposals
library(parallel)   #   these libraries
library(foreach)    #   for running chains
library(doParallel) #   in parallel
library(coda)       #   for mcmcm diagnostics
library(sn)         # to have a skew normal prior
timeToMinutes <- function(time){
# purpose : Converts times in the twelve hour clock system to the relevant
#           minute of the day
# input   : time - Character representation of time of day in the HH:MM format
#                  (no seconds)
# output  : integer from 1 to 1440
if (class(time)!='character') stop('invalid input')
# It's far easier to do simple arithmatic if we turn all 12 O'Clocks to 0s:
time <- sub("^12","0",time)
# Figure out if we're in the first half or second half of the day, and add
# the correct number of minutes accordingly:
output <- ifelse(grepl("AM$", time), 0, 720)
# We use some simple regular expressions to chop off the content before
# and after the part of the string which indicates the minutes:
mins <- time %>% sub(pattern="[AP]M$", replacement="") %>%
sub(pattern="^[0-9]+:", replacement="") %>% as.numeric()
# likewise for the hours:
hours <- time %>% sub(pattern=":[0-9]+[AP]M$", replacement="") %>%
as.numeric()
output <- output + mins + hours*60
return(output)
}
# Function to reorder a data.frame so that the last column is now the first:
reorder <- function(x) subset(x, select=c(dim(x)[2],1:dim(x)[2]-1))
#################### METROPOLIS HASTINGS IMPLEMENTATION ########################
normProp <- function(xt, propPars){
mean <- xt
sd <- propPars
return(rnorm(1, mean, sd))
}
normTarg <- function(x, pars){
mean <- pars[1]
sd <- pars[2]
return(log(dnorm(x, mean, sd)))
}
multvarNormProp <- function(xt, propPars){
return(rmvnorm(1, mean=xt, sigma=propPars))
}
multvarNormTarg <- function(x, pars){
mean <- pars$mean
sigma <- pars$sigma
return(log(dmvnorm(x, mean=mean, sigma=sigma)))
}
multvar.uFunc <- function(chain){
# purpose : takes as input the output of an MCMCM chain, and uses this
#           to estimate the parameters for a multivariate normal proposal
# input   : The matrix output of an MCMCM chain, where the first column
#           is the log target evaluated at the sample point
# output  : an nxn covariance matrix, where the chain was run in n
#           dimensions
return(cov(chain[,-1]))
}
MH <- function(proposal, propPars, lTarg, lTargPars, x0, itermax=1000,
uFunc=NULL, prntPars=FALSE){
# purpose : Adaptive Metropolis hastings MCMC
# inputs  : proposal  - A function which generates proposals for new points
#           propPars  - Parameters for the proposal distribution
#           lTarg     - A function which can evaluate the log target
#           lTargPars - The parameters of the log-target
#           x0        - A chosen point at which to commence to algorithm
#           itermax   - The maximum number of points to propose
#           uFunc     - If an update function is provided, adaptive MCMC
#                       will be used instead.
# output  : A list of points generated by the MCMC algorithm
n <- length(x0)+1
xPrev <- x0
# if there's no update function to do adaptive MCMC, we simply run the chain
# normally and return the result:
if (is.null(uFunc)){
chain <- runChain(itermax, proposal, propPars, xPrev, lTarg, lTargPars, n)
return(chain)
}
# otherwise, we perform adaptive MCMC:
else{
output <- matrix(c(lTarg(xPrev, lTargPars), xPrev), nrow=1, byrow=T)
# We perform the MCMC in three phases. An inital warmup phase (lasting 25%
# of itermax), follwed by an adaptation phase, where we use the warmup
# samples to modify our proposal, sample 25% of itermax more samples and
# then modify the proposal again. The final stage uses the newest proposal
# to produce samples for the last 50% of itermax iterations
# Split itermax into 2 groups of 25% and one group of 50%:
div <- c(round((itermax-1)/4))
indices <- c(div, div, itermax-2*div)
for (im in indices){
# Optionally display the proposal parameters:
if (prntPars) print(propPars)
# run the chain:
chain <- runChain(im, proposal, propPars, xPrev, lTarg, lTargPars, n)
# add the samples to the output matrix
output <- rbind(output, chain)
# This line simply breaks out the loop if we don't have a following
# phase, to avoid the computational cost of updating our parameters:
if(dim(output)[1]>=itermax) break
# update the proposal parameters for the next phase:
propPars <- uFunc(chain)
# update xPrev for the next phase:
xPrev <- chain[im,-1]
}
return(output)
}
}
pMH <- function(proposal, propPars, lTarg, lTargPars, x0, itermax=1000,
uFunc=NULL, prntPars=FALSE, nChains=4, clNeeds){
# purpose : wrapper function which runs MCMC chains in parallel
# inputs  : nChains - The number of chains which should be run
#           other   - for all other inputs, refer to the description given
#                     in the MH function.
# output  : A list containing the output matrices of all chains
# Create the clusters and initiate paralellisation:
cl<-makeCluster(detectCores())
registerDoParallel(cl)
clusterExport(cl, c(c("MH","runChain"),clNeeds))
# Do the parallel loop:
ls <- foreach(i=1:nChains) %dopar% {
to.ls <- MH(proposal, propPars, lTarg, lTargPars, x0, itermax,
uFunc, prntPars)
}
# Do the tear down for the parallelisation, we exception handle it simply
# to silence a bunch of warnings that indicate which connections to unused
# ports are closed by stopping the cluster
try(stopCluster(cl), silent = T)
class(ls) <- 'pMCMCoutput'
return(ls)
}
runChain <- function(itermax, proposal, propPars, xPrev, lTarg, lTargPars, n){
# purpose : subroutine of MH, which performs the running of the MCMC chain.
#           Using this subroutine allows for a cleaner implementation of
#           adaptive metropolis hastings
output <- matrix(NA, nrow=itermax, ncol=n)
# Add the initial point, the final column of the output matrix is an
# evaluation of the log Target at the sampled point:
output[1,] <- c(lTarg(xPrev, lTargPars), xPrev)
lTargOld <-  lTarg(xPrev, lTargPars)
for (t in 2:itermax){
# generate a proposed point:
xNew <- proposal(xPrev, propPars)
# calculate the acceptance probability:
lTargNew <- lTarg(xNew, lTargPars)
#lTargOld <- output[t-1,1]
alpha <- exp(lTargNew - lTargOld)
# determine acceptance or rejection:
u <- runif(1)
if (u<=alpha){
output[t,] <- c(lTargNew, xNew)
lTargOld <- lTargNew
}
else output[t,] <- c(lTargOld, xPrev)
# update Xt-1
xPrev <- output[t,2:n]
}
return(output)
}
yhat <- function(ti, theta){
# purpose : calculates the expected temmperature at a given time point,
#           given the parameters theta
# inputs  : ti    - The temperature of the ith observation. Can also be a
#                   vector
#           theta - The parameters of the cubic equation which we use to
#                   model the change of temperature throughout the day
# output  : A vector (maybe of length one) of the expected temperature at the
#           given times, given theta
return(theta[1]*ti^3 + theta[2]*ti^2 + theta[3]*ti + theta[4])
}
llik.cubic.reg <- function(data, pars){
# purpose : Evaluates the log likelihood of our observations, given the
#           observation model (expected temperature follows a cubic. Realised
#           temperatures are normally distributed around this predicted value
#           with same variance for all temperatures).
# inputs  : data - a matrix of n rows and two columns. Each row is an
#                  observation. The first column is the temperature we
#                  observed, the second is the time of day at which we
#                  observed this temperature
#           pars - the model parameters. This should be a list with
#                  $theta and $sigma. theta is the vector of parameters
#                  for the cubic regression, sigma is the standard deviation
#                  of the normal distribution of points around this curve
# outputs : A vector of evaluated likelihoods at the given data points
temps <- data[,2]
times <- data[,1]
theta <- pars$theta
sigma <- pars$sigma
mu <- yhat(times, theta)
# The below assigns a different mean to the normal for each observed
# temperature (its associated expected temperature) and assumes the same
# sigma. We then sum these logs to get the liklihood of all the points,
# assuming they are independent (clearly not the case, but some concessions
# must be made):
llikelihood <- sum(log(dnorm(temps,mu,sigma)))
# cat('SS:',sum(expected.temps-temps)^2,'\n')
# cat('llik:',llikelihood,'\n')
return(llikelihood)
}
lcubicTarget <- function(x, lTargPars){
# purpose : Evaluates the log posterior for a model where the change of
#           temperature throughout the day follows a cubic distribution
# inputs  : x        - the parameters of the cubic, and the standard deviation
#                      of the normal noise around the cubic
#           TargPars - A list containing the functions which evlaute the
#                      $priors for the parameters, as well as the $data being
#                      used to fit the model, and the $priorPars
# output  : The evaluated log posterior distribution, given the data
# In this case, because of how the MCMC function is written, the x is
# a vector of proposed parameter values, and the lTargPars are really the
# data points we have to fit our model.
# we know that we have 5 parameters, 4 for the regression, and a sigma:
theta <- x[1:4]
sigma <- exp(x[5])
data <- lTargPars$data
# lpriors should be a list with 5 elements, all functions, which evaluate the
# log prior probability of the five parameters, respectively:
priors <- lTargPars$priors
priorPars <- lTargPars$priorPars
# Tried doing this more cleanly with a for loop but it bugged for some very
# strange reasons. The uniform prior on theta would return a value other
# than 0 for values outside the support, didn't figure out was causing this
# in the for loop as opposed to hard coding it:
p1 <- do.call(priors[[1]], c(list(x[1]), priorPars[[1]]))
p2 <- do.call(priors[[2]], c(list(x[2]), priorPars[[2]]))
p3 <- do.call(priors[[3]], c(list(x[3]), priorPars[[3]]))
p4 <- do.call(priors[[4]], c(list(x[4]), priorPars[[4]]))
p5 <- do.call(priors[[5]], c(list(exp(x[5])), priorPars[[5]]))
evaledlPriors <- log(p1) + log(p2) + log(p3) + log(p4) + log(p5)
if (evaledlPriors==-Inf) return(-Inf)
else return(evaledlPriors +
llik.cubic.reg(data=data,pars=list(theta=theta, sigma=sigma)))
}
plot.pMCMCoutput <- function(chains, names, ...){
# purpose : Draws diagnostic plots of an MCMC chain given the sampled points
# input   : The output of the MCMC chain, containing the sampled points,
#           and the evaluation of the log target at each sampled point, and
#           a vector of names for the pairwise plots
# output  : returns nothing, but produces plots
plotCol <- function(x) plot(x, type='l', col='blue')
plotACF <- function(x) plot(acf(x, plot=F, ...)$acf,
type='h',col='blue',ylab='',xlab='')
prevMfrow <- par('mfrow')
for(chain in chains){
parNum <- dim(chain)[2]-1
samps <-  dim(chain)[1]
par(mfrow=c(parNum,1))
apply(chain[,-1], 2, plotCol)
apply(chain[,-1], 2, plotACF)
class(chain) <- "matrix"
df <- data.frame(chain[runif(1000,1,nrow(chain)),])
names(df) <- names
plot(df, pch=16,col=adjustcolor('black', alpha=0.2))
}
# reset the graphical parameters to what the were before the call:
par(mfrow=prevMfrow)
}
thinMCMC <- function(pMCMCoutput, alpha=0.1, removeBI=TRUE){
# purpose : takes the output of the pHM function, and thins the chain to the
#           desired level
# inputs  : pMCMCMoutput - the output list of chains from the pMH function
#           alpha        - the percentage of points to keep
#           removeBI     - before thinning, should we remove all the points
#                          from the burn in period?
nchains <- length(pMCMCoutput)
for (i in 1:nchains){
chain <- pMCMCoutput[[i]]
if (removeBI) pMCMCoutput[[i]] <- chain[round(nrow(chain)/2):nrow(chain),]
n <- nrow(pMCMCoutput[[i]])
toKeep <- round(seq(1,n,length=round(alpha*n)))
pMCMCoutput[[i]] <- pMCMCoutput[[i]][toKeep,]
}
return(pMCMCoutput)
}
getMinMax <- function(theta){
# purpose : Given a set of parameter values for a cubic, returns the minimum
#           and maximum values of that function in the x axis range 0 to 1
# inputs  : theta - the cubic parameters
# output  : a vector containing the minimum and maximum value in the interval
# calculate where the local minimum and maximum will be:
discriminant <- try(sqrt(theta[2]^2-3*theta[1]*theta[3]), silent=T)
# if the equation has no real solutions, then the maximum can't be in the
# given range on the real plane:
if (class(discriminant)=='try-error') localMaxs <- c(-1,2)
else localMaxs <- (-theta[2] + c(discriminant,-discriminant))/(3*theta[1])
# remove points if they are outside the 0 1 window:
localMaxs <- localMaxs[which(localMaxs > 0 & localMaxs < 1)]
vals <- yhat(c(0,1,localMaxs), theta)
return(c(min(vals), max(vals)))
}
llikModel <- function(data, pars){
# purpose : Evaluates the probability of observing the min and max temps we
#           saw, given the model parameters
# inputs  : data - a matrix of n rows and two columns. Each row is an
#                  observation. The first column is the minimum temperature
#                  observed, the second is the maximum
#           pars - the model parameters. This should be a list with
#                  $theta and $sigma. theta is the vector of parameters
#                  for the cubic function, sigma is the standard deviation
#                  of the normal distribution of points around this curve.
#                  Should also contain $dayTemp, the average value of the
#                  the temperature on the day of the observations
# outputs : A vector of evaluated likelihoods at the given data points
# obtain the predicted mins and maxs for the data:
preds <- getMinMax(pars$theta) + pars$dayTemp
minLik <- dnorm(data[,1], preds[1], pars$sigma)
maxLik <- dnorm(data[,2], preds[2], pars$sigma)
return(sum(log(minLik) + log(maxLik)))
}
lmodelTarget <- function(x, lTargPars){
# purpose : Evaluates the log posterior for a model where the min and max temp
#           in a given day are normally distributed, with temperature change
#           throughout the day being a cubic function
# inputs  : x        - the parameters of the cubic, the standard deviation
#                      of the normal noise around the cubic, and the value of
#                      average temperature on the day of the data
#           TargPars - A list containing the functions which evlaute the
#                      $priors for the parameters, as well as the $data being
#                      used to fit the model, and the $priorPars
# output  : The evaluated log posterior distribution, given the data
# In this case, because of how the MCMC function is written, the x is
# a vector of proposed parameter values, and the lTargPars are really the
# data points we have to fit our model.
theta <- x[1:4]
sigma <- exp(x[5]) # ensure sigma positive, force the MCMC to the log scale
daytemp <- x[6]
data <- lTargPars$data
priors <- lTargPars$priors
priorPars <- lTargPars$priorPars
thetaPrior <- priors[[1]](theta, mean=priorPars[[1]]$mean,
sigma=priorPars[[1]]$sigma)
sigmaPrior <- priors[[2]](sigma, priorPars[[2]]$shape)
daytempPrior <- priors[[3]](daytemp, dp=priorPars[[3]]$dp)
evaledlPriors <- log(thetaPrior) + log(sigmaPrior) + log(daytempPrior)
pars=list(theta=theta, sigma=sigma, dayTemp=daytemp)
return(evaledlPriors + llikModel(data=data,pars=pars))
}
optimModelWrapper <- function(x, lTargPars){
return(-lmodelTarget(x, lTargPars))
}
drawCurves <- function(data, SingleChain, howmany){
# purpose : overlays cubic curves given the sampled parameters of the
#           posterior distribution
# inputs  : data        - The data to be ploted along with the points
#           SingleChain - The output of a single MCMC chain
#           howmany     - How many curves should be drawn?
addCurve <- function(x){
lines(seq(0,1,length=1000),yhat(seq(0,1,length=1000), theta=x[2:5]),
col=adjustcolor('blue', alpha=0.2))
}
# plot the points:
plot(data$NT, tempdata$Temp, pch=16, col=adjustcolor('black',alpha=0.2),
xlab='Time of day from midnight to midnight',
ylab='Temperature in Celcius')
# add the curves:
indices <- round(seq(1,nrow(SingleChain),length=howmany))
apply(SingleChain[indices,], 1, addCurve)
}
ProbBelZero <- function(singleChain){
# purpose : calculates the posterior probability that the temperature is below
#           0 at midday, given a set of samples from the posterior
#           distribution of the main model
# input   : The output of a single markov chain
n <- nrow(singleChain)
belowZ <-  function(x) pnorm(0, yhat(0.5, x[2:5]) + x[7], exp(x[6]))
return(apply(singleChain,1,belowZ))
}
ProporBelowZero <- function(singleChain, pointsToUse=1e4){
}
# The txt files have no column names, so we create them here:
vars <- c('Time','Temp','DewPoint','Humidity','WindDir','WindSpeed',
'WindGust','Pressure','Precip','PrecipAccum','Condition',
'year','day')
# We create a loop which goes through the collection of files, and pastes them
# all together in one large data.frame. We add in a couple columns with the year
# and day for each observation too. The data consist of hourly observations on
# each day:
data <- NA
counter <- 0
for (year in 0:4){
for (day in 1:7){
# generate the name we know the file will have, based on the year and day:
filename <- paste('201',year,'-1-',day,'cleaned.txt',sep='')
# load the data and add the 'year' and 'day' columns:
current.data <- read.table(filename, header=F)
n <- dim(current.data)[1]
current.data <- cbind(current.data, rep(paste('201',year,sep=''), n))
current.data <- cbind(current.data, rep(day, n))
names(current.data) <- vars
# update the data.frame with the data from the current file:
if (counter==0) data <- current.data
else data <- rbind(data, current.data)
counter <- counter + 1
}
}
# want to get an idea of the distribution of minimal and maximal temps accross
# these time periods:
Year <- Day <- Min <- Max <- vector()
# We create a loop which goes through the collection of files, and pastes them
# all together in one large data.frame. We add in a couple columns with the year
# and day for each observation too. The data consist of hourly observations on
# each day:
data <- NA
counter <- 0
for (year in 0:4){
for (day in 1:7){
# generate the name we know the file will have, based on the year and day:
filename <- paste('201',year,'-1-',day,'cleaned.txt',sep='')
# load the data and add the 'year' and 'day' columns:
current.data <- read.table(filename, header=F)
n <- dim(current.data)[1]
current.data <- cbind(current.data, rep(paste('201',year,sep=''), n))
current.data <- cbind(current.data, rep(day, n))
names(current.data) <- vars
# update the data.frame with the data from the current file:
if (counter==0) data <- current.data
else data <- rbind(data, current.data)
counter <- counter + 1
}
}
mat(1:4, nrow = 2, by.row = T)
matrix(1:4, nrow = 2, by.row = T)
matrix(1:4, nrow = 2, byrow = T)
matrix(1:4, nrow = 2, byrow = T) %*% matrix(5:8, nrow = 2, byrow = T)
0.96 + 0.5*(1.93-4*0.96)
# For the pipe operator:
library(magrittr)
data("example_data")
head(example_data)
library(GAI)
data("example_data")
head(example_data)
# The GAI package comes with a handy function to convert data.frame data to
# a matrix of counts, to facilitate plotting. This function also checks if
# the survey has any missing occasions, so can be a useful way of sanitising
# a new dataset:
test_data <- extract_counts(example_data[-5,], returnDF = F)
head(test_data)
# The GAI package comes with a handy function to convert data.frame data to
# a matrix of counts, to facilitate plotting. This function also checks if
# the survey has any missing occasions, so can be a useful way of sanitising
# a new dataset:
test_data <- extract_counts(example_data[-5,], returnDF = T)
build()
library(devtools)
build()
setwd("~/github-cfj/GAI/R")
build()
install()
devtools::install_github("r-glennie/openpopscr", build = TRUE,
build_opts = c("--no-resave-data", "--no-manual"),
build_vignettes = TRUE)
install.packages("markovchain")
devtools::install_github('spedygiorgio/markovchain')
library(installr)
updateR()
updateR()
installed.packages()
devtools::install_github("r-glennie/openpopscr", build = TRUE, build_opts = c("--no-resave-data", "--no-manual"), build_vignettes = TRUE)
